üîç Coding Self-Attention in PyTorch
A minimal, easy-to-understand implementation of Self-Attention using PyTorch ‚Äî inspired by the legendary paper "Attention is All You Need". This repository is designed for learners who want to grasp the mechanics of self-attention by walking through every step, line by line.

üß† What‚Äôs Inside?
A lightweight SelfAttention module built from scratch using PyTorch's nn.Module.

Manual calculation of queries, keys, and values with no bias terms (just like the original paper).

Hands-on walkthrough of how scaled dot-product attention works, including:

Computing similarity scores.

Applying softmax for attention weights.

Generating context-aware embeddings.

A small and interpretable example (with d_model=2) so you can follow the math by hand.

Printouts of intermediate steps (like weight matrices, attention scores, etc.) to verify correctness.


üí° Why This Project?
This project is perfect for:

Students and beginners exploring Transformers.

Practitioners who want to truly understand how attention works under the hood.

Educators needing a clear, reproducible example to demonstrate self-attention.
